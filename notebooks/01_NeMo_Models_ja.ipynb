{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASnx4b5jXsil"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install text-unidecode\n",
        "\n",
        "# ## Install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "\n",
        "## Install TorchAudio\n",
        "!pip install torchaudio>=0.10.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "## Grab the config we'll use in this example\n",
        "!mkdir configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0eAURFKXdFT"
      },
      "source": [
        "# minGPT License",
        "\n",
        "*このノートブックでは、[minGPTコードベース](https://github.com/karpathy/minGPT)を同等のNeMoコードに移植しています。したがって、minGPTのライセンスはここに添付されています。*",
        "\n",
        "```\n",
        "MIT License (MIT) Copyright (c) 2020 Andrej Karpathy",
        "\n",
        "ここに、本ソフトウェアおよび関連文書ファイル（以下「本ソフトウェア」という）の複製を入手した者に対し、制限なく本ソフトウェアを使用するための無償の許可が与えられる。これには、使用、複製、改変、結合、公開、配布、サブライセンス、および／または本ソフトウェアのコピーを販売するための権利が含まれ、また、本ソフトウェアを受領した者に対してもこれらの行為を許可する。ただし以下の条件に従うものとする：",
        "\n",
        "上記の著作権表示および本使用許諾条件は、本ソフトウェアのすべてのコピーまたは重要な部分に含めなければならない。",
        "\n",
        "本ソフトウェアは「現状のまま」提供され、明示的または黙示的を問わず、商品適格性、特定目的への適合性、および非侵害性を含むがこれらに限定されない、あらゆる種類の保証が一切付されていません。いかなる場合においても、著者または著作権者は、契約違反、不法行為その他のいかなる法的理論に基づく場合であっても、本ソフトウェアに関連して生じるいかなる請求、損害またはその他の責任についても、一切の責任を負わないものとします。",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b7Z064UZFH9"
      },
      "source": [
        "# torch-rnn License",
        "*このノートブックでは、[torch-rnn](https://github.com/jcjohnson/torch-rnn) コードベースの `tiny-shakespeare` データセットを使用しています。したがって、torch-rnn のライセンスはここに添付されています。*",
        "\n",
        "```\n",
        "MITライセンス (MIT)",
        "\n",
        "Copyright (c) 2016 Justin Johnson",
        "\n",
        "ここに、本ソフトウェアおよび関連文書ファイル（以下「本ソフトウェア」という）の複製を入手した者に対し、制限なく本ソフトウェアを使用するための無償の許可が与えられる。これには、使用、複製、改変、結合、公開、配布、サブライセンス、および／または本ソフトウェアのコピーを販売するための権利が含まれ、また、本ソフトウェアを受領した者に対してもこれらの行為を許可する。ただし以下の条件に従うものとする：",
        "\n",
        "上記の著作権表示および本使用許諾条件は、本ソフトウェアのすべてのコピーまたは重要な部分に含めなければならない。",
        "\n",
        "本ソフトウェアは「現状のまま」提供され、明示的または黙示的を問わず、商品適格性、特定目的への適合性、および非侵害性を含むがこれらに限定されない、あらゆる種類の保証が一切付されていません。いかなる場合においても、著者または著作権者は、契約違反、不法行為その他のいかなる法的理論に基づく場合であっても、本ソフトウェアに関連して生じるいかなる請求、損害またはその他の責任についても、一切の責任を負わないものとします。",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKzK-Z7obCED"
      },
      "source": [
        "-------",
        "\n",
        "***注意: このノートブックでは、`[ERROR CELL]` とマークされたセル内でニューラルタイプやモデル開発の概念の威力を示すため、意図的にエラーを導入します。このようなエラーの説明と解決方法は、後続のセルに記載されています。***",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81qdv0mPee-j"
      },
      "source": [
        "# The NeMo Model",
        "\n",
        "NeMoには、ユーザーが独自のデータセットで迅速にトレーニングとファインチューニングを開始できるよう、複数の最先端の事前学習済み会話型AIモデルが用意されています。",
        "\n",
        "前回の[NeMo入門](https://colab.research.google.com/github/NVIDIA/NeMo/blob/stable/tutorials/00_NeMo_Primer.ipynb)ノートブックでは、NeMoを使用して事前学習済みチェックポイントをダウンロードする方法を学び、NeMoモデルの基本概念についても解説しました。前回のチュートリアルでは、NeMoモデルの使用方法、修正方法、保存方法、および復元方法について説明しました。",
        "\n",
        "このチュートリアルでは、ゼロから非凡なNeMoモデルを開発する方法を学びます。これにより、基盤となるコンポーネントの理解と、それらがPyTorchエコシステム全体とどのように相互作用するかを理解するのに役立ちます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKNftwxzllth"
      },
      "source": [
        "-------",
        "NeMoの中核をなすのは「Model」の概念です。NeMo開発者にとって、「Model」は以下の要素を統合した単一の統一ユニットを指します：\n- ニューラルネットワーク自体\n- それらのネットワークをサポートするインフラストラクチャ\n- これらを単一の統合ユニットとしてパッケージ化したもの\nこのため、ほとんどのNeMoモデルはデフォルトで以下の機能を備えています（注：一部のNeMoモデルでは、特定のドメイン/ユースケースに特化した追加機能をサポートしています！） - \n\n[Note: The original text was in Japanese and provided a conceptual explanation of \"Model\" in NeMo. The English translation maintains the original structure and content while using natural English phrasing.]",
        "\n",
        "- ニューラルネットワークアーキテクチャ - モデルに必要なすべてのモジュール。",
        "\n",
        "-  データセット + データローダー - 訓練や評価時にデータを消費可能な形式に準備するためのすべてのコンポーネント。",
        "\n",
        "-  前処理 + 後処理 - データセットを処理し、モジュールが容易にデータを消費できるようにするコンポーネントのいずれか。",
        "\n",
        "-  Optimizer + Schedulers - デフォルト設定としてすぐに使用可能で、追加の実験も容易に行える基本設定です。",
        "\n",
        "- その他の補助インフラストラクチャ - トークナイザー、言語モデル構成、データ拡張など"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VOoAQT1mipO"
      },
      "source": [
        "# NeMoモデルの構築",
        "\n",
        "NeMoの「モデル」は複数の主要コンポーネントで構成されているため、それらを一つずつ解説していきます。上記の記載順序に従って説明を試みます。",
        "\n",
        "少し難易度を上げるために、今回は自然言語処理分野のモデルを移植してみましょう。Transformerモデルは現在非常に人気があり、BERTやセサミストリート出身の仲間たちは、多くの自然言語処理タスクの中核インフラを形成しています。",
        "\n",
        "NeMoモデルに期待される主要なコンポーネントを簡潔かつ包括的に解説した優れた実装例（ただし簡潔な実装）が、`minGPT`リポジトリで公開されています - https://github.com/karpathy/minGPT。スクリプト自体は短いものの、NeMoモデルに期待される主要なコンポーネントを分かりやすく簡潔に解説しているため、NeMoモデルの開発プロセスを深く理解するための優れた教材と言えます。補足：NeMoはNLPコレクションの一環としてGPTをサポートしており、本ノートブックはこのようなモデルの開発プロセスを深く理解するための詳細な開発解説を目的としています。",
        "\n",
        "以下のノートブックでは、minGPTをNeMoに移植する試みを行い、その過程でNeMo自体の中核的な概念について議論します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOlQKsaRot1l"
      },
      "source": [
        "# ニューラルネットワークアーキテクチャの構築",
        "\n",
        "まず第一に、リストの筆頭に来るのはNeMoモデルの中核を成すニューラルネットワークです。",
        "\n",
        "では、このようなモデルをどのように作成するのでしょうか？PyTorchを使用します！以下でご覧いただけるように、NeMoコンポーネントはすべてのPyTorchと互換性があるため、ワークフローを拡張してもPyTorch自体の柔軟性を失うことはありません！",
        "\n",
        "まずはいくつかのインポートから始めましょう："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piLOgwOPX1FS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nemo\n",
        "from nemo.core import NeuralModule\n",
        "from nemo.core import typecheck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yySYjHgAqVvT"
      },
      "source": [
        "## ニューラルモジュール",
        "待って、`NeuralModule`って何？ あの素晴らしい`torch.nn.Module`はどこ？",
        "\n",
        "`NeuralModule`は`torch.nn.Module`のサブクラスであり、いくつかの追加機能を備えています。",
        "\n",
        "`torch.nn.Module`として完全にPyTorchエコシステムと互換性があるだけでなく、以下の機能を備えています -",
        "\n",
        "1) `Typing` - この機能はモデルに `Neural Type Checking` のサポートを追加します。`Typing` はオプション機能ですが、後述するように非常に有用です！",
        "\n",
        "2) `Serialization` - Remember the `OmegaConf` config dictionary and YAML config files? Well, all `NeuralModules` inherently supports serialization/deserialization from such config dictionaries!",
        "\n",
        "3) `FileIO` - これは完全にオプションのファイルシリアライゼーションシステムです。`NeuralModule` が PyTorch チェックポイントに保存できないデータを保存する方法を必要としている場合、シリアライゼーションとデシリアライゼーションのロジックを2つの便利なメソッドに記述してください！**注意**: 最終的なNeMoモデルを作成する際には、この処理は自動的に実装されます！NeMoモデルの自動シリアライゼーション/デシリアライゼーションサポート！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bseLiNoqqQrE"
      },
      "outputs": [],
      "source": [
        "class MyEmptyModule(NeuralModule):\n",
        "\n",
        "  def forward(self):\n",
        "    print(\"Neural Module ~ hello world!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4Q36L5urdOQ"
      },
      "outputs": [],
      "source": [
        "x = MyEmptyModule()\n",
        "x()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXAcn5Ot_1I"
      },
      "source": [
        "## ニューラルタイプ",
        "\n",
        "ニューラルタイプ？その用語が何を指すのか疑問に思っているかもしれませんね。",
        "\n",
        "ほぼすべてのNeMoコンポーネントはクラス`Typing`を継承しています。`Typing`はシンプルなクラスで、継承元クラスに2つのプロパティを追加します：`input_types`と`output_types`。最も簡潔に定義すると、NeuralTypeは単なる意味的テンソルです。テンソルが保持すべき意味的形状に関する情報と、そのテンソルが表す意味的情報を含んでいます。以上です。",
        "\n",
        "では、このような型付きテンソルにはどのような意味情報が含まれているのでしょうか？以下に具体例を示します。",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezOJERbVwG34"
      },
      "source": [
        "------",
        "深層学習分野では、テンソルの形状は一致しているにもかかわらず、意味論的には全く一致していないケースにしばしば遭遇します。例えば以下のランク3テンソルをご覧ください -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvC57bbxwXxN"
      },
      "outputs": [],
      "source": [
        "# Case 1:\n",
        "embedding = torch.nn.Embedding(num_embeddings=10, embedding_dim=30)\n",
        "x = torch.randint(high=10, size=(1, 5))\n",
        "print(\"x :\", x)\n",
        "print(\"embedding(x) :\", embedding(x).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMaqhMBgxe2C"
      },
      "outputs": [],
      "source": [
        "# Case 2\n",
        "lstm = torch.nn.LSTM(1, 30, batch_first=True)\n",
        "x = torch.randn(1, 5, 1)\n",
        "print(\"x :\", x)\n",
        "print(\"lstm(x) :\", lstm(x)[0].shape)  # Let's take all timestep outputs of the LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IQHjki-yezX"
      },
      "source": [
        "-------",
        "ご覧の通り、ケース1の出力は形状が[1, 5, 30]の埋め込み表現であり、ケース2の出力はLSTMの出力状態`h`（全時間ステップにわたる）であり、これも同じ形状[1, 5, 30]です。",
        "\n",
        "それらは同じ形状ですか？ **はい**。<br>\nCase1 .shape == Case2 .shape を比較した場合、出力結果は True になりますか？ **はい**。<br>",
        "それらは同じ概念を表していますか？ **いいえ**。<br>",
        "\n",
        "\n",
        "これら2つのテンソルが同じ意味情報を表現していないことを認識できる能力こそが、私たちがニューラルタイプを活用する理由です。これにはテンソルが表現する形状情報と、その意味概念の両方が含まれています。これら2つのテンソルの出力間でニューラルタイプチェックを行った場合、それらが意味的に異なるものであるというエラーが発生します（より技術的に言えば、それらは互いに`INCOMPATIBLE`であると判定されます）！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucP0hNI7vWrU"
      },
      "source": [
        "--------",
        "\n",
        "[Named Tensors](https://pytorch.org/docs/stable/named_tensor.html) のような概念について聞いたことがあるかもしれません。概念的には似ていますが、NeMoが付与するNeural TypesはPyTorchエコシステムとそれほど緊密に結びついていません。実際、クラスの任意のオブジェクトにNeural Typeを付与することができます！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvf5oLt9zxSS"
      },
      "source": [
        "## ニューラルタイプ - 使用法",
        "\n",
        "ニューラルタイプは興味深い概念ですが、実際に実装するにはどうすればよいでしょうか？以下にいくつかのケースを考えてみましょう。",
        "\n",
        "ニューラルタイプはNeMoの中核的な基盤の一つであり、大多数のニューラルモジュールで使用され、すべてのNeMoモデルではニューラルタイプが定義されます。これらは完全にオプションであり侵入的ではありませんが、NeMoはコンポーネント間の意味的な互換性を確保するためにそれを強力にサポートしています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTizOBUg0qIB"
      },
      "source": [
        "型チェックされたモジュールの基本的な例から始めましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp0FG8NJt1Jd"
      },
      "outputs": [],
      "source": [
        "from nemo.core.neural_types import NeuralType\n",
        "from nemo.core.neural_types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tsgs8Fp0-WV"
      },
      "outputs": [],
      "source": [
        "class EmbeddingModule(NeuralModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = torch.nn.Embedding(num_embeddings=10, embedding_dim=30)\n",
        "\n",
        "  @typecheck()\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x)\n",
        "\n",
        "  @property\n",
        "  def input_types(self):\n",
        "    return {\n",
        "        'x': NeuralType(axes=('B', 'T'), elements_type=Index())\n",
        "    }\n",
        "\n",
        "  @property\n",
        "  def output_types(self):\n",
        "    return {\n",
        "        'y': NeuralType(axes=('B', 'T', 'C'), elements_type=EmbeddedTextType())\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY9GYEoD3Yy0"
      },
      "source": [
        "ニューラルタイプの有用性を示すため、上記のケースをNeuralModules内で再現します。",
        "\n",
        "上記クラスに型チェック機能を追加する方法について議論しましょう。",
        "\n",
        "1) `forward` には型チェック用のデコレータ `@typecheck()` が付与されています。",
        "\n",
        "2) `input_types` と `output_types` プロパティが定義されています。",
        "\n",
        "それで終わりです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on268fAX4LLU"
      },
      "source": [
        "-------",
        "\n",
        "上記の各ステップについて詳しく説明しましょう。",
        "\n",
        "- `@typecheck()` はシンプルなデコレータで、`Typing` を継承した任意のクラス (NeuralModule が自動的にこれを処理します) に対して、デフォルトで `input_types` と `output_types` という2つのデフォルトプロパティを追加します。これらのプロパティはデフォルトで None を返します。",
        "\n",
        "`@typecheck()` デコレータを明示的に使用することで、デフォルトではニューラル型チェックが**無効化**されます。NeMo はモデル開発プロセスに干渉することを望んでいません。そのため、ユーザーは 2 つのプロパティをオーバーライドすることで型チェックを「オプトイン」できます。したがって、このデコレータは、ユーザーが型チェックを必要としない限り、型チェックによって負担を強いられることがないようにします。",
        "\n",
        "`@typecheck()` とは一体何でしょうか？端的に言えば、これは `Typing` を継承したクラスの任意の関数をラップするために使用できるデコレータです。これにより、そのクラスの型定義を自動的に参照し、強制適用します。通常、`torch.nn.Module` のサブクラスでは通常 `forward()` メソッドしか実装しないため、このメソッドをラップするのが一般的ですが、`@typecheck()` は非常に柔軟なデコレータです。NeMo では、特定のドメイン（TTS など）で非常に重要な高度な使用例をいくつか紹介します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9i1KugG5om7"
      },
      "source": [
        "------",
        "\n",
        "前述の通り、`@typecheck()` は型強制を行います。では、NeMo にこのような型情報をどのように提供すればよいのでしょうか？",
        "\n",
        "クラスの `input_types` と `output_types` プロパティをオーバーライドすることで、文字列名を `NeuralType` にマッピングする辞書を返すことができます。",
        "\n",
        "上記の場合、`NeuralType`を以下の2つの要素で定義します：",
        "\n",
        "- `axes`: これは軸自体が持つ意味情報です。最も一般的な軸情報は単一文字表記によるものです。",
        "\n",
        "> `B` = バッチ<br>",
        "> `C` / `D` - チャネル / 次元（同一扱い） <br>",
        "> `T` - 時間 <br>",
        "> `H` / `W` - 高さ / 幅 <br>",
        "\n",
        "- `elements_type`: これは「テンソルが表すもの」の意味情報です。すべてのこのような型は基本クラス `ElementType` から派生しており、単に `ElementType` をサブクラス化するだけで、NeMo で使用可能なカスタム意味型の階層構造を構築できます！",
        "\n",
        "ここで、入力は `Index` 型の `ElementType` (語彙内の文字のインデックス) であり、出力は `EmbeddedTextType` 型の `ElementType` (テキスト埋め込み) であると宣言します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boxxMniv27vi"
      },
      "outputs": [],
      "source": [
        "embedding_module = EmbeddingModule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgfDuBm27wiV"
      },
      "source": [
        "次に、上記のケース2に相当するものを`NeuralModule`として構築してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZZOOoCJ2-iV"
      },
      "outputs": [],
      "source": [
        "class LSTMModule(NeuralModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lstm = torch.nn.LSTM(1, 30, batch_first=True)\n",
        "\n",
        "  @typecheck()\n",
        "  def forward(self, x):\n",
        "    return self.lstm(x)\n",
        "\n",
        "  @property\n",
        "  def input_types(self):\n",
        "    return {\n",
        "        'x': NeuralType(axes=('B', 'T', 'C'), elements_type=SpectrogramType())\n",
        "    }\n",
        "\n",
        "  @property\n",
        "  def output_types(self):\n",
        "    return {\n",
        "        'y': NeuralType(axes=('B', 'T', 'C'), elements_type=EncodedRepresentation())\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iIWIunz8IQq"
      },
      "source": [
        "------",
        "ここでは、上記ケース2のLSTMモジュールを定義します。",
        "\n",
        "入力をランク3のテンソルに変更し、これは現在「スペクトログラムタイプ」を表現しています。意図的に汎用的に設計しており、入力として`MelSpectrogramType`や`MFCCSpectrogramType`としても使用可能です！",
        "\n",
        "LSTMの出力は現在`EncodedRepresentation`となっています。実際には、CNN層の出力、Transformerブロックの出力、あるいはこの場合はLSTM層の出力として扱うことができます。もちろん、EncodedRepresentationをサブクラス化して独自の実装を作成することも可能です！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LlOJf0C8GN4"
      },
      "outputs": [],
      "source": [
        "lstm_module = LSTMModule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj0wonSz8_0c"
      },
      "source": [
        "------",
        "さあ、テストを始めましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giLJlub78-Ja"
      },
      "outputs": [],
      "source": [
        "# Case 1 [ERROR CELL]\n",
        "x1 = torch.randint(high=10, size=(1, 5))\n",
        "print(\"x :\", x1)\n",
        "print(\"embedding(x) :\", embedding_module(x1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-fhclja9WLr"
      },
      "source": [
        "-----",
        "なぜ最初から `TypeError` が発生するのか疑問に思うかもしれません。この `TypeError` は意図的に発生するように設計されています。",
        "\n",
        "位置引数はモデル開発時に重大な問題を引き起こす可能性があり、特にモデル/モジュール設計が確定していない場合に問題が発生しやすくなります。誤った位置引数によるエラーの可能性を減らし、関数に渡される引数の名前を強制するために、`Typing`では**すべての型チェック済み関数をkwargsのみを使用して呼び出すこと**を要求しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KUj_p6M9L-f"
      },
      "outputs": [],
      "source": [
        "# Case 1\n",
        "print(\"x :\", x1)\n",
        "print(\"embedding(x) :\", embedding_module(x=x1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dirhWWvMRusx"
      },
      "source": [
        "では、ケース2の`LSTMModule`についても同様に試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMu3B0-9-CqE"
      },
      "outputs": [],
      "source": [
        "# Case 2 [ERROR CELL]\n",
        "x2 = torch.randn(1, 5, 1)  # Input = [B=1, T=5, C=1]\n",
        "print(\"x :\", x2)\n",
        "print(\"lstm(x) :\", lstm_module(x=x2)[0].shape)  # Let's take all timestep outputs of the LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OTLdR_4-isV"
      },
      "source": [
        "-----",
        "今度は、提供された出力引数の数が期待される数と一致しないという型エラーが発生します。",
        "\n",
        "ここで具体的に何が行われているのでしょうか？ 私たちの `LSTMModule` クラス内部では、出力タイプを単一のニューラルタイプ - 形状が [B, T, C] の `EncodedRepresentation` と宣言しています。",
        "\n",
        "しかし、LSTM層の出力は以下のようなタプルです：",
        "1) [B, T, C] 形状のエンコード表現",
        "2) もう一つのタプルで、隠れ状態`h`とセル状態`c`の2つの状態値を含んでいます。それぞれの形状は[num_layers * num_directions, B, C]です！",
        "\n",
        "このため、ニューラル型システムは出力引数の数が期待される値と一致しないことを示すエラーを発生させます。",
        "\n",
        "**注**: 2つの状態の軸タイプ情報は `D` で表されます。これは一般的な「次元」を表すためです。`num_layers` と `num_directions` は単一の軸に統合されているためです。NeMoにおいて、`C` と `D` の軸タイプは同等であり、相互に置き換え可能であるため、ここでは `C` を使用してLSTMの隠れ次元を表し、`D` を使用して統合された軸 `num_layers * num_directions` を表します。",
        "\n",
        "上記を修正しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2u-keAM-d-B"
      },
      "outputs": [],
      "source": [
        "class CorrectLSTMModule(LSTMModule):  # Let's inherit the wrong class to make it easy to override\n",
        "  @property\n",
        "  def output_types(self):\n",
        "    return {\n",
        "        'y': NeuralType(axes=('B', 'T', 'C'), elements_type=EncodedRepresentation()),\n",
        "        'h_c': [NeuralType(axes=('D', 'B', 'C'), elements_type=EncodedRepresentation())],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a99NX0O8KMvW"
      },
      "source": [
        "`h_c` ニューラルタイプについては、リスト `[]` でラップする必要があります。NeMo はデフォルトで、各 `NeuralType` が単一の戻り値に対応すると仮定しています。ただし、LSTM の場合、2つの状態テンソルからなるタプルを生成します。",
        "\n",
        "そこで、NeMoに対してこの特定の`NeuralType`が単一次元のアイテムリストであり、このリストの各要素が同じ`NeuralType`を持ち、同じ形状であることを通知します。",
        "\n",
        "NeMoはその後、`h_c`が常にテンソルのリストであることを保証します。リスト内のアイテムの数はチェックしませんが、返される値が*ゼロ個以上のアイテムを含むリスト*であること、そしてこれらの各アイテムが同じ`NeuralType`を共有していることを保証します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyPZH-fz_dG4"
      },
      "outputs": [],
      "source": [
        "lstm_module = CorrectLSTMModule()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9whH50PE_Xyx"
      },
      "outputs": [],
      "source": [
        "# Case 2\n",
        "x2 = torch.randn(1, 5, 1)\n",
        "y2, (h, c) = lstm_module(x=x2)\n",
        "print(\"x :\", x2)\n",
        "print(\"lstm(x) :\", y2.shape)  # The output of the LSTM RNN\n",
        "print(\"hidden state (h) :\", h.shape)  # The first hidden state of the LSTM RNN\n",
        "print(\"hidden state (c) :\", c.shape)  # The second hidden state of the LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRueNvNY_jI3"
      },
      "source": [
        "------",
        "素晴らしい！これで型チェックシステムは満足しています。",
        "\n",
        "よく見ると、これらの出力は通常のTorchテンソルです（これは良いニュースです。結局のところ、Torchテンソルと互換性を持たせたくないですからね！）では、具体的にどのような情報が格納されているのでしょうか？",
        "\n",
        "`output_types`がオーバーライドされ、有効なtorchテンソルが結果として返される場合、これらのテンソルには`neural_type`属性が付加されます。これを確認してみましょう -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGQ9XbWU_ffa"
      },
      "outputs": [],
      "source": [
        "emb_out = embedding_module(x=x1)\n",
        "lstm_out = lstm_module(x=x2)[0]\n",
        "\n",
        "assert hasattr(emb_out, 'neural_type')\n",
        "assert hasattr(lstm_out, 'neural_type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEpBruSOScPJ"
      },
      "outputs": [],
      "source": [
        "print(\"Embedding tensor :\", emb_out.neural_type)\n",
        "print(\"LSTM tensor :\", lstm_out.neural_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWTsqiAHAony"
      },
      "source": [
        "-------",
        "このように、これらのテンソルには新たに `neural_type` という属性が追加され、形状も同じになっています。",
        "\n",
        "この演習の目的は、たとえ形状が同じであっても、二つの出力が意味的に**同一のオブジェクトではない**ことを主張することにありました。",
        "\n",
        "これをテストしてみよう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AU9FMtdATIm"
      },
      "outputs": [],
      "source": [
        "emb_out.neural_type.compare(lstm_out.neural_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cqnqAGIBCjA"
      },
      "outputs": [],
      "source": [
        "emb_out.neural_type == lstm_out.neural_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmH6B0mHDJqb"
      },
      "source": [
        "## ニューラルタイプ - 制約事項",
        "\n",
        "興味深いことに、私たちの入力は型付き関数呼び出しの両方において単なる `torch.Tensor` であり、それらには `neural_type` が割り当てられていませんでした。",
        "\n",
        "では、型チェックシステムはなぜエラーを検出しなかったのでしょうか？",
        "\n",
        "これは互換性を維持するためです - 型チェックは関数呼び出しの連鎖において機能するように設計されています - そして各関数自体も `@typecheck()` デコレータでラップされるべきです。これはまた、フォワードコールに数十ものチェックを課すことを避けるためであり、したがって私たちは高次の論理計算を実行するモジュールのみを型付けします。",
        "\n",
        "------",
        "\n",
        "具体例として、ResNetモデルの各残差ブロックの入力と出力を個別に記述する必要はほとんどありません（ただし可能です）。しかし、エンコーダ（内部の層数にかかわらず）およびデコーダ（分類ヘッド）を個別に記述することは実用的に重要です。これにより、ファインチューニングを行う際に、エンコーダに入力されるテンソルとデコーダにバインドされるテンソルの間に意味的な不一致が生じないようにできます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m28zSEKEjt_"
      },
      "source": [
        "-------",
        "このケースでは、クラスを拡張して入力テンソルに型を付加するのは非現実的であるため、ショートカットとして直接入力テンソルにニューラル型を付加することができます！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGbKB4gJEzcU"
      },
      "outputs": [],
      "source": [
        "embedding_module = EmbeddingModule()\n",
        "x1 = torch.randint(high=10, size=(1, 5))\n",
        "\n",
        "# Attach correct neural type\n",
        "x1.neural_type = NeuralType(('B', 'T'), Index())\n",
        "\n",
        "print(\"embedding(x) :\", embedding_module(x=x1).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0j-evylFM5j"
      },
      "outputs": [],
      "source": [
        "# Attach wrong neural type [ERROR CELL]\n",
        "x1.neural_type = NeuralType(('B', 'T'), LabelsType())\n",
        "\n",
        "print(\"embedding(x) :\", embedding_module(x=x1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StMPyg6oCC9B"
      },
      "source": [
        "## minGPTコンポーネントを作成しよう",
        "\n",
        "ニューラル型チェックについてある程度理解が深まったところで、minGPTのサンプルコードの移植を開始しましょう。ここでも、コードの大部分は[minGPTリポジトリ](https://github.com/karpathy/minGPT)からそのまま移植する形で進めます。",
        "\n",
        "ここで注目すべき点があります。クラスインポートを変更するだけで、`@typecheck()`を前方に適用し、`input_types`と`output_types`を追加するだけで（これらは完全にオプションです！）、PyTorch Lightningへの移植作業はほぼ完了です！",
        "\n",
        "**注意**: GPTコンポーネントクラスをすべてヘルパーモジュールに移動しました。これは、NeMoのセキュリティ検証プロセスにおける`__main__`名前空間の問題を回避するためです。以下の方法でインポートしてください:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from helper_files.gpt_components import (\n",
        "    AttentionType, SelfAttentionType, CausalSelfAttentionType,\n",
        "    CausalSelfAttention, Block,\n",
        "    GPTEmbedding, GPTTransformerEncoder, GPTDecoder\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raFkuSRaBAE0"
      },
      "outputs": [],
      "source": [
        "# Basic imports needed for the tutorial\n",
        "import math\n",
        "from typing import List, Set, Dict, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yakGOXrzF1XW"
      },
      "source": [
        "## 要素タイプの作成",
        "\n",
        "これまで私たちは NeMo コアが提供するニューラルタイプを使用してきました。しかし、事前に定義された要素タイプに限定される必要はありません！",
        "\n",
        "ユーザーは自由に、任意の階層構造を持つ要素タイプを定義できます！",
        "\n",
        "補助モジュールでは、注意機構関連のニューラルタイプの階層構造を作成する `AttentionType`、`SelfAttentionType`、`CausalSelfAttentionType` といったカスタム要素タイプを定義しています。これらのタイプは `helper_files.gpt_components` からインポートされています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybhLLVyUF0mo"
      },
      "outputs": [],
      "source": [
        "# Custom element types are now imported from helper_files.gpt_components:\n",
        "# - AttentionType(EncodedRepresentation): Basic Attention Element Type\n",
        "# - SelfAttentionType(AttentionType): Self Attention Element Type\n",
        "# - CausalSelfAttentionType(SelfAttentionType): Causal Self Attention Element Type\n",
        "print(\"Custom element types imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mONJRMdbZNSE"
      },
      "source": [
        "## モジュールの作成",
        "\n",
        "ニューラルモジュールは通常最上位レベルのモジュールですが、モジュール階層の任意のレベルで使用することができます。",
        "\n",
        "デモンストレーションとして、因果自己注意モジュールのブロックから構成されるエンコーダを型付きニューラルモジュールとして扱います。もちろん、各因果自己注意層そのものをニューラルモジュールとして扱うことも可能ですが、一般的には最上位モジュールが好まれます。",
        "\n",
        "基本的な PyTorch モジュール (`CausalSelfAttention` と `Block`) は、NeMo のセキュリティ検証で発生する `__main__` 名前空間の問題を回避するため、ヘルパーモジュールからインポートされています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4oXpAL_CoDp"
      },
      "outputs": [],
      "source": [
        "# CausalSelfAttention and Block classes are now imported from helper_files.gpt_components\n",
        "# These are standard PyTorch nn.Module implementations:\n",
        "# - CausalSelfAttention: A vanilla multi-head masked self-attention layer\n",
        "# - Block: An unassuming Transformer block combining attention and MLP\n",
        "\n",
        "print(\"Basic PyTorch modules imported successfully!\")\n",
        "print(f\"CausalSelfAttention: {CausalSelfAttention}\")\n",
        "print(f\"Block: {Block}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv0dyrLifkw0"
      },
      "source": [
        "## NeMoモデルの構築",
        "\n",
        "NeMoモデルは様々なコンポーネントで構成されているため、このノートブック内で段階的にモデルを構築していきます。その結果、複数の中間段階のNeMo「モデル」が生成され、これらは部分的な実装となり、相互に反復的に継承されることになります。",
        "\n",
        "NeMoモデル（NeMoコレクションに含まれるもの）を完全に実装する場合、これらのコンポーネントは通常単一のクラス内に配置されます。",
        "\n",
        "まず、PyTorch NeMo Modelの中核クラスである`ModelPT`を継承しましょう。これはPyTorch Lightning Moduleを継承したクラスです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxeG-qMrRgNU"
      },
      "source": [
        "-------",
        "**Remember**:",
        "\n",
        "- `torch.nn.Module` に相当する NeMo のクラスは `NeuralModule` です。",
        "- `LightningModule`に相当する`ModelPT`クラスが用意されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TsfmCYthMux"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as ptl\n",
        "from nemo.core import ModelPT\n",
        "from omegaconf import OmegaConf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ib2rSz2hjaP"
      },
      "source": [
        "------",
        "次に、NeMoモデルの最小限の実装を構築しましょう。具体的にはコンストラクタ、重みの初期化メソッド、およびforwardメソッドを実装します。",
        "\n",
        "まずはminGPT実装で採用されている手順に従い、その後NeMo向けに段階的にリファクタリングを行う。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98x9-Fh-HVwj"
      },
      "outputs": [],
      "source": [
        "class PTLGPT(ptl.LightningModule):\n",
        "  def __init__(self,\n",
        "                 # model definition args\n",
        "                 vocab_size: int, # size of the vocabulary (number of possible tokens)\n",
        "                 block_size: int, # length of the model's context window in time\n",
        "                 n_layer: int, # depth of the model; number of Transformer blocks in sequence\n",
        "                 n_embd: int, # the \"width\" of the model, number of channels in each Transformer\n",
        "                 n_head: int, # number of heads in each multi-head attention inside each Transformer block\n",
        "                 # model optimization args\n",
        "                 learning_rate: float = 3e-4, # the base learning rate of the model\n",
        "                 weight_decay: float = 0.1, # amount of regularizing L2 weight decay on MatMul ops\n",
        "                 betas: Tuple[float, float] = (0.9, 0.95), # momentum terms (betas) for the Adam optimizer\n",
        "                 embd_pdrop: float = 0.1, # \\in [0,1]: amount of dropout on input embeddings\n",
        "                 resid_pdrop: float = 0.1, # \\in [0,1]: amount of dropout in each residual connection\n",
        "                 attn_pdrop: float = 0.1, # \\in [0,1]: amount of dropout on the attention matrix\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        # save these for optimizer init later\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.betas = betas\n",
        "\n",
        "        # input embedding stem: drop(content + position)\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
        "        self.drop = nn.Dropout(embd_pdrop)\n",
        "        # deep transformer: just a sequence of transformer blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, block_size, n_head, attn_pdrop, resid_pdrop) for _ in range(n_layer)])\n",
        "        # decoder: at the end one more layernorm and decode the answers\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False) # no need for extra bias due to one in ln_f\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        print(\"number of parameters: %e\" % sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "  def forward(self, idx):\n",
        "      b, t = idx.size()\n",
        "      assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "      # forward the GPT model\n",
        "      token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "      position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "      x = self.drop(token_embeddings + position_embeddings)\n",
        "      x = self.blocks(x)\n",
        "      x = self.ln_f(x)\n",
        "      logits = self.head(x)\n",
        "\n",
        "      return logits\n",
        "\n",
        "  def get_block_size(self):\n",
        "      return self.block_size\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      \"\"\"\n",
        "      Vanilla model initialization:\n",
        "      - all MatMul weights \\in N(0, 0.02) and biases to zero\n",
        "      - all LayerNorm post-normalization scaling set to identity, so weight=1, bias=0\n",
        "      \"\"\"\n",
        "      if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "          module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "          if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "              module.bias.data.zero_()\n",
        "      elif isinstance(module, nn.LayerNorm):\n",
        "          module.bias.data.zero_()\n",
        "          module.weight.data.fill_(1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bMf5SO7wmor"
      },
      "source": [
        "------",
        "念のため、上記のPyTorch Lightningモデルを作成して動作を確認しましょう !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrXIBzg4wutC"
      },
      "outputs": [],
      "source": [
        "m = PTLGPT(vocab_size=100, block_size=32, n_layer=1, n_embd=32, n_head=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCcgn1bajPW8"
      },
      "source": [
        "------",
        "それでは、上記のコードを簡単にNeMoモデルに変換してみましょう。",
        "\n",
        "NeMoモデルコンストラクタは通常、以下の2つの要素のみを受け入れます：",
        "\n",
        "1) `cfg`: OmegaConf DictConfigオブジェクトで、モデルがニューラルネットワークアーキテクチャ、データローダー設定、オプティマイザ設定、およびモデル自体に必要なその他のコンポーネントを定義するために使用する正確な構成要素を指定します。",
        "\n",
        "2) `trainer`: PyTorch LightningのオプションのTrainerオブジェクトで、NeMoモデルをトレーニングに使用する場合に指定できます。構築後（必要に応じて）に`set_trainer`メソッドを使用して設定できます。このノートブックでは、Trainerオブジェクトの設定は行いません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQMTCB3kz0UA"
      },
      "source": [
        "## ニューラルモジュールのリファクタリング",
        "\n",
        "前述の通り、ニューラルモジュールは一般にモデルの上位レベル構成要素であり、同等のニューラルモジュールで置き換えることが可能である。",
        "\n",
        "上述の通り、埋め込みモジュール、深層トランスフォーマーデコーダネットワーク、および最終デコーダ層はすべて、PyTorch Lightning実装のコンストラクタ内で統合されています。",
        "\n",
        "------",
        "\n",
        "ただし、最終デコーダーモジュールは単純な線形層ではなく、RNNで構成されていた可能性もあります。あるいは、1次元CNNが採用されていた可能性もあります。",
        "\n",
        "同様に、深層トランスフォーマーデコーダでは、Self Attentionモジュールの実装が異なる可能性がある。",
        "\n",
        "これらの変更は、上記の実装内で直接実装するのは容易ではありません。しかし、これらのコンポーネントをそれぞれ独立したNeuralModuleにリファクタリングすれば、将来的に構築する同等のモジュールで簡単に置き換えることが可能になります！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJj5sSkX0xHi"
      },
      "source": [
        "### 埋め込みモジュールのリファクタリング",
        "\n",
        "まず、上記の実装から埋め込みモジュールをリファクタリングしましょう。`GPTEmbedding`クラスは、ヘルパーモジュールからインポートされています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYwMyjqK05RL"
      },
      "outputs": [],
      "source": [
        "# GPTEmbedding NeuralModule is now imported from helper_files.gpt_components\n",
        "# It implements token and positional embeddings with dropout\n",
        "print(f\"GPTEmbedding imported: {GPTEmbedding}\")\n",
        "\n",
        "# Example instantiation (with dummy parameters for demonstration)\n",
        "dummy_embedding = GPTEmbedding(vocab_size=100, n_embd=32, block_size=128)\n",
        "print(f\"Input types: {dummy_embedding.input_types}\")\n",
        "print(f\"Output types: {dummy_embedding.output_types}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5rOP6lyOyRt"
      },
      "source": [
        "### エンコーダのリファクタリング",
        "\n",
        "次に、GPTエンコーダのリファクタリングを行います。これはマルチレイヤーTransformer（Decoder）ネットワークとして実装されています。`GPTTransformerEncoder`クラスは、現在補助モジュールからインポートされています。",
        "\n",
        "------",
        "ここで言及しているのはGPTの「Encoder」モジュールですが、実際にはTransformerの「Decoder」ブロックを用いて構築されています。",
        "\n",
        "***「ニューラルモジュール」について議論する際、我々は特定の入力ニューラルタイプと特定の出力ニューラルタイプを備えた抽象的なモジュールについて議論しているのである。***",
        "\n",
        "私たちにとって、GPT「Encoder」ニューラルモジュールは、任意の実装を受け入れるように設計されており、その",
        "\n",
        "- 入力ニューラルタイプは `NeuralType(('B', 'T', 'C'), EmbeddedTextType())` です",
        "\n",
        "- 出力型は `NeuralType(('B', 'T', 'C'), EncodedRepresentation())` です",
        "\n",
        "-----",
        "このようなGPT「エンコーダ」ニューラルモジュールの具体的な実装例として、Deep Transformer「デコーダ」ネットワークが挙げられる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QeQnQ_G2PwH"
      },
      "outputs": [],
      "source": [
        "# GPTTransformerEncoder NeuralModule is now imported from helper_files.gpt_components\n",
        "# It implements a sequence of transformer blocks for encoding\n",
        "print(f\"GPTTransformerEncoder imported: {GPTTransformerEncoder}\")\n",
        "\n",
        "# Example instantiation (with dummy parameters for demonstration)\n",
        "dummy_encoder = GPTTransformerEncoder(n_embd=32, block_size=128, n_head=4, n_layer=1)\n",
        "print(f\"Input types: {dummy_encoder.input_types}\")\n",
        "print(f\"Output types: {dummy_encoder.output_types}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmCR3LK3QHum"
      },
      "source": [
        "### デコーダのリファクタリング",
        "\n",
        "最後に、解答をデコードするための小規模な単一層フィードフォワードネットワークであるDecoderをリファクタリングしましょう。`GPTDecoder`クラスは、ヘルパーモジュールからインポートされています。",
        "\n",
        "-------",
        "\n",
        "注目すべき点として、Decoderの`input_types`は汎用的な`EncoderRepresentation()`を受け入れますが、`GPTTransformerEncoder`の`neural_type`は`output_type`として`CausalSelfAttentionType`を採用しています。",
        "\n",
        "これは意味論的にはミスマッチではありません！上記の継承関係図でご覧いただけるように、`EncodedRepresentation` -> `AttentionType` -> `SelfAttentionType` -> `CausalSelfAttentionType` という継承関係を宣言しています。",
        "\n",
        "この`element_type`の継承階層により、将来のエンコーダ（少なくとも`EncodedRepresentation`型のニューラル出力を持つエンコーダ）が、現在のGPT因果自己注意エンコーダと置き換え可能になります。これにより、NeMoモデルのその他の部分は問題なく動作し続けます！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCPUu0EWQIBX"
      },
      "outputs": [],
      "source": [
        "# GPTDecoder NeuralModule is now imported from helper_files.gpt_components\n",
        "# It implements layer normalization followed by a linear layer to produce logits\n",
        "print(f\"GPTDecoder imported: {GPTDecoder}\")\n",
        "\n",
        "# Example instantiation (with dummy parameters for demonstration)\n",
        "dummy_decoder = GPTDecoder(n_embd=32, vocab_size=100)\n",
        "print(f\"Input types: {dummy_decoder.input_types}\")\n",
        "print(f\"Output types: {dummy_decoder.output_types}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYLMjlW0Sdy1"
      },
      "source": [
        "### NeMo GPTモデルのリファクタリング",
        "\n",
        "埋め込み層、エンコーダ、デコーダ用にそれぞれ3つのNeuralModuleを実装したので、このリファクタリングを活用してNeMoモデルを再構成しましょう！",
        "\n",
        "今回は汎用の`LightningModule`ではなく、`ModelPT`を継承しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQlmtYU6iDwi"
      },
      "outputs": [],
      "source": [
        "class AbstractNeMoGPT(ModelPT):\n",
        "  def __init__(self, cfg: OmegaConf, trainer: ptl.Trainer = None):\n",
        "      super().__init__(cfg=cfg, trainer=trainer)\n",
        "\n",
        "      # input embedding stem: drop(content + position)\n",
        "      self.embedding = self.from_config_dict(self.cfg.embedding)\n",
        "      # deep transformer: just a sequence of transformer blocks\n",
        "      self.encoder = self.from_config_dict(self.cfg.encoder)\n",
        "      # decoder: at the end one more layernorm and decode the answers\n",
        "      self.decoder = self.from_config_dict(self.cfg.decoder)\n",
        "\n",
        "      self.block_size = self.cfg.embedding.block_size\n",
        "      self.apply(self._init_weights)\n",
        "\n",
        "      print(\"number of parameters: %e\" % self.num_weights)\n",
        "\n",
        "  @typecheck()\n",
        "  def forward(self, idx):\n",
        "      b, t = idx.size()\n",
        "      assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "      # forward the GPT model\n",
        "      # Remember: Only kwargs are allowed !\n",
        "      e = self.embedding(idx=idx)\n",
        "      x = self.encoder(embed=e)\n",
        "      logits = self.decoder(encoding=x)\n",
        "\n",
        "      return logits\n",
        "\n",
        "  def get_block_size(self):\n",
        "      return self.block_size\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      \"\"\"\n",
        "      Vanilla model initialization:\n",
        "      - all MatMul weights \\in N(0, 0.02) and biases to zero\n",
        "      - all LayerNorm post-normalization scaling set to identity, so weight=1, bias=0\n",
        "      \"\"\"\n",
        "      if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "          module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "          if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "              module.bias.data.zero_()\n",
        "      elif isinstance(module, nn.LayerNorm):\n",
        "          module.bias.data.zero_()\n",
        "          module.weight.data.fill_(1.0)\n",
        "\n",
        "  @property\n",
        "  def input_types(self):\n",
        "    return {\n",
        "        'idx': NeuralType(('B', 'T'), Index())\n",
        "    }\n",
        "\n",
        "  @property\n",
        "  def output_types(self):\n",
        "    return {\n",
        "        'logits': NeuralType(('B', 'T', 'C'), LogitsType())\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFRmxWiSmdF3"
      },
      "source": [
        "## モデル用の設定ファイルを作成する",
        "\n",
        "一見すると、上記のPyTorch Lightning実装と比べて特に変更はありません。コンストラクタがconfigオブジェクトを受け入れるようになった点を除いて、まったく変更はありません！",
        "\n",
        "NeMoモデルは、対応する設定辞書（OmegaConfオブジェクトとしてインスタンス化）と共に動作します。この仕組みにより、Hydraを迅速に活用してモデルのプロトタイプ作成が可能になります。その他にも、ハイパーパラメータ最適化やNeMoモデルのシリアライズ/デシリアライズなど、様々な利点があります。",
        "\n",
        "実際にこのような設定オブジェクトを構築する方法を見てみましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uygo0BEYjKuj"
      },
      "outputs": [],
      "source": [
        "# model definition args (required)\n",
        "# ================================\n",
        "# vocab_size: int # size of the vocabulary (number of possible tokens)\n",
        "# block_size: int # length of the model's context window in time\n",
        "# n_layer: int # depth of the model; number of Transformer blocks in sequence\n",
        "# n_embd: int # the \"width\" of the model, number of channels in each Transformer\n",
        "# n_head: int # number of heads in each multi-head attention inside each Transformer block\n",
        "\n",
        "# model definition args (optional)\n",
        "# ================================\n",
        "# embd_pdrop: float = 0.1, # \\in [0,1]: amount of dropout on input embeddings\n",
        "# resid_pdrop: float = 0.1, # \\in [0,1]: amount of dropout in each residual connection\n",
        "# attn_pdrop: float = 0.1, # \\in [0,1]: amount of dropout on the attention matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4sdqRAFop-n"
      },
      "source": [
        "------",
        "上記の必須パラメータを確認すると、OmegaConfに対してこれらの値が現在設定されていないが、ユーザーが使用する前に設定すべきであることを示す方法が必要です。",
        "\n",
        "OmegaConf では `MISSING` 値を使用してこのような動作をサポートしています。YAML 設定ファイルでは、プレースホルダーとして `???` を使用することで同様の効果が得られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqLSZq7Soo2j"
      },
      "outputs": [],
      "source": [
        "from omegaconf import MISSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTH-1vu8TO7o"
      },
      "outputs": [],
      "source": [
        "# Let's create a utility for building the class path\n",
        "def get_class_path(cls):\n",
        "  return f'{cls.__module__}.{cls.__name__}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xToaWAJUmtX"
      },
      "source": [
        "### Model configの構造",
        "\n",
        "まず、モデルレベルの設定ファイルで使用する共通コンポーネントの設定を作成しましょう："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCvLdOlMVLy_"
      },
      "outputs": [],
      "source": [
        "common_config = OmegaConf.create({\n",
        "    'vocab_size': MISSING,\n",
        "    'block_size': MISSING,\n",
        "    'n_layer': MISSING,\n",
        "    'n_embd': MISSING,\n",
        "    'n_head': MISSING,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8hvdKa4VmCV"
      },
      "source": [
        "-----",
        "現在のモデル設定はまだ構築中です。もっと詳細な情報を追加する必要があります！",
        "\n",
        "完全な Model Config には、最上位モジュールのすべてのサブ構成も含める必要があります。具体的には、`embedding`、`encoder`、および `decoder` の各構成が含まれます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-2_QOZyVgrE"
      },
      "source": [
        "### サブモジュール設定の構造",
        "\n",
        "最上位モデルの場合、実際のモジュール自体を変更することはほとんどなく、代わりにそのモデルのハイパーパラメータを主に変更します。",
        "\n",
        "そこで、`Hydra`のクラスインスタンス化メソッドを活用します。このメソッドは、クラスメソッド`ModelPT.from_config_dict()`を通じて簡単にアクセスできます。",
        "\n",
        "以下にいくつかの具体例を挙げましょう -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntsxQKH0pDac"
      },
      "outputs": [],
      "source": [
        "embedding_config = OmegaConf.create({\n",
        "    '_target_': get_class_path(GPTEmbedding),\n",
        "    'vocab_size': '${model.vocab_size}',\n",
        "    'n_embd': '${model.n_embd}',\n",
        "    'block_size': '${model.block_size}',\n",
        "    'embd_pdrop': 0.1\n",
        "})\n",
        "\n",
        "encoder_config = OmegaConf.create({\n",
        "    '_target_': get_class_path(GPTTransformerEncoder),\n",
        "    'n_embd': '${model.n_embd}',\n",
        "    'block_size': '${model.block_size}',\n",
        "    'n_head': '${model.n_head}',\n",
        "    'n_layer': '${model.n_layer}',\n",
        "    'attn_pdrop': 0.1,\n",
        "    'resid_pdrop': 0.1\n",
        "})\n",
        "\n",
        "decoder_config = OmegaConf.create({\n",
        "    '_target_': get_class_path(GPTDecoder),\n",
        "    # n_embd: int, vocab_size: int\n",
        "    'n_embd': '${model.n_embd}',\n",
        "    'vocab_size': '${model.vocab_size}'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtloTqkqWhpl"
      },
      "source": [
        "##### `_target_`とは何ですか？",
        "--------",
        "\n",
        "上記の設定では、configファイル内に`_target_`が指定されています。`_target_`は通常、Pythonパッケージまたはユーザーローカルディレクトリ内の実際のクラスへの完全なクラスパスを指定します。Hydraがこのパスからモデルを正しく検索してインスタンス化するために必要です。",
        "\n",
        "では、なぜクラスパスを設定したいのでしょうか？",
        "\n",
        "一般的にモデル開発においては、エンコーダやデコーダ自体を変更することはあまりありませんが、エンコーダとデコーダのハイパーパラメータを変更することはよく行われます。",
        "\n",
        "この表記法は、モデルレベルの順方向処理の宣言を簡潔かつ正確に記述するのに役立ちます。また、モデルのどの部分を容易に置き換えられるかを論理的に示すこともできます。将来的には、エンコーダを他のタイプの自己注意ブロックに、あるいはデコーダをRNNや1D-CNNニューラルモジュールに容易に置き換えることが可能です（ただし、これらのモジュールは現在のブロックと同じNeural Type定義を持っている必要があります）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASDmcgE4XtQ4"
      },
      "source": [
        "##### `${}` 構文とは何ですか？",
        "-------",
        "\n",
        "OmegaConf、および間接的にHydraも変数補間をサポートしています。埋め込み層、エンコーダ、デコーダのニューラルモジュールの`__init__`メソッドでご覧いただけるように、これらのモジュール間では多くのパラメータが共有されています。",
        "\n",
        "各埋め込み層、エンコーダ、デコーダの設定でこれらのコンストラクタの値を個別に設定するのは、煩雑でエラーが発生しやすい作業になります。",
        "\n",
        "代わりに、`model`レベルの設定内で標準キーを定義し、それぞれの設定内でこれらの値を補間します！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXvEcXGhZi5I"
      },
      "source": [
        "### モデルとモジュールレベルの設定の添付",
        "\n",
        "現在、コアコンポーネントにはモデルレベルとモジュールごとの設定が用意されています。サブモジュールの設定は通常「model」名前空間に属しますが、必要に応じて独自の構造を定義することも可能です。",
        "\n",
        "それらを貼り付けましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8hvNeB_aDgi"
      },
      "outputs": [],
      "source": [
        "model_config = OmegaConf.create({\n",
        "    'model': common_config\n",
        "})\n",
        "\n",
        "# Then let's attach the sub-module configs\n",
        "model_config.model.embedding = embedding_config\n",
        "model_config.model.encoder = encoder_config\n",
        "model_config.model.decoder = decoder_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIubuFcOpIB0"
      },
      "source": [
        "-----",
        "この設定を印刷しましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SyKNgp9pG0N"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(model_config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PAA07EAauCn"
      },
      "source": [
        "-----",
        "待って、OmegaConf はなぜまだ設定ファイルの変数展開値を自動的に埋め込んでくれないの？",
        "\n",
        "これは、OmegaConfが変数補間に対して遅延評価アプローチを採用しているためです。まず必要なフィールドの仮値を設定します（`???`でマークされたフィールド）。その後、事前に解決を強制するには、以下のスニペットを使用できます -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X4C76JyOAnN"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugxA0TPtbHVZ"
      },
      "outputs": [],
      "source": [
        "temp_config = copy.deepcopy(model_config)\n",
        "temp_config.model.vocab_size = 10\n",
        "temp_config.model.block_size = 4\n",
        "temp_config.model.n_layer = 1\n",
        "temp_config.model.n_embd = 32\n",
        "temp_config.model.n_head = 4\n",
        "\n",
        "temp_config = OmegaConf.create(OmegaConf.to_container(temp_config, resolve=True))\n",
        "print(OmegaConf.to_yaml(temp_config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V41RFIpEpiOu"
      },
      "source": [
        "-----",
        "これで設定が完了したので、NeMoモデルのオブジェクトを作成してみましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIIVi2IfpsJ4"
      },
      "outputs": [],
      "source": [
        "# Let's work on a copy of the model config and update it before we send it into the Model.\n",
        "cfg = copy.deepcopy(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OllBhswPqQXq"
      },
      "outputs": [],
      "source": [
        "# Let's set the values of the config (for some plausible small model)\n",
        "cfg.model.vocab_size = 100\n",
        "cfg.model.block_size = 128\n",
        "cfg.model.n_layer = 1\n",
        "cfg.model.n_embd = 32\n",
        "cfg.model.n_head = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJm2LnTqqcIM"
      },
      "outputs": [],
      "source": [
        "print(OmegaConf.to_yaml(cfg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7tpB8BcqeBO"
      },
      "outputs": [],
      "source": [
        "# Try to create a model with this config [ERROR CELL]\n",
        "m = AbstractNeMoGPT(cfg.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXOLhpxdq4Ni"
      },
      "source": [
        "-----",
        "\n",
        "この NeMo Model に `Abstract` タグを追加した理由は、このモデルをインスタンス化しようとすると、特定のメソッドを実装する必要があることを示すエラーが発生するためです。",
        "\n",
        "1) `setup_training_data` & `setup_validation_data` - すべての NeMo モデルは、トレーニングデータローダーと検証データローダーという 2 つのデータローダーを実装する必要があります。オプションとして、`setup_test_data` メソッドも実装することで、モデル単体での評価をサポートすることもできます。",
        "\n",
        "この制約を設ける理由は、NeMo Modelsが統一された整合性のあるオブジェクトとして設計されているためです。このオブジェクトには、対応するモデルの基盤となるニューラルネットワークの詳細や、モデルの訓練・検証・オプションとしてのテストに使用するデータローダーに関する情報が含まれます。",
        "\n",
        "この処理において、モデルが作成/デシリアライズされた後、モデルをゼロから訓練する、あるいはユーザーが提供した任意のデータセットに対してモデルの微調整/評価を行うには、ユーザー提供のデータセットがこのモデルで使用するデータセット/データローダーがサポートする形式である必要があります！",
        "\n",
        "2) `list_available_models` - これはクラウドからユーザーに事前学習済みNeMoモデルのリストを提供するユーティリティメソッドです。",
        "\n",
        "通常、NeMoモデルはtarファイルに簡単にパッケージ化できます（この種のtarファイルを私たちは「.nemoファイル」と呼んでいます）。これらのtarファイルにはモデルの設定ファイルと、Modelの事前学習済みチェックポイント重みが含まれており、クラウドサービスから簡単にダウンロードできます。",
        "\n",
        "このノートブックでは、この手法を実装しません。",
        "\n",
        "--------",
        "最後に、上記のNeMoモデルの具体的な実装を作成してみましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcwi1lO7t7Sm"
      },
      "outputs": [],
      "source": [
        "from nemo.core.classes.common import PretrainedModelInfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckCxyVLYqrz0"
      },
      "outputs": [],
      "source": [
        "class BasicNeMoGPT(AbstractNeMoGPT):\n",
        "\n",
        "  @classmethod\n",
        "  def list_available_models(cls) -> PretrainedModelInfo:\n",
        "    return None\n",
        "\n",
        "  def setup_training_data(self, train_data_config: OmegaConf):\n",
        "    self._train_dl = None\n",
        "\n",
        "  def setup_validation_data(self, val_data_config: OmegaConf):\n",
        "    self._validation_dl = None\n",
        "\n",
        "  def setup_test_data(self, test_data_config: OmegaConf):\n",
        "    self._test_dl = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofUoJ8DDvq_Y"
      },
      "source": [
        "------",
        "では、`BasicNeMoGPT`モデルのオブジェクトを作成してみましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8iYQSC5vptU"
      },
      "outputs": [],
      "source": [
        "m = BasicNeMoGPT(cfg.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otvYW4TBxAju"
      },
      "source": [
        "## 訓練・検証・テストのステップ設定",
        "\n",
        "上記の `BasicNeMoGPT` モデルは、基本的な PyTorch Lightning モジュールであり、以下の追加機能を備えています：",
        "\n",
        "1) ニューラルタイプチェックのサポート - モデル定義および内部モジュールで定義されたニューラルタイプチェックをサポートします。",
        "\n",
        "2) モデルの保存と復元（単純なケース）を tar ファイルに保存する。",
        "\n",
        "ただし、現在の実装ではこのモデルはPyTorch Lightningの`Trainer`クラスをサポートしていません。このため、このモデルは手動で呼び出すことは可能ですが、PyTorch Lightningフレームワークを使用して簡単に訓練したり評価したりすることはできません。",
        "\n",
        "------",
        "\n",
        "それではこの機能のサポートを追加していきましょう -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU3oQAVovxRg"
      },
      "outputs": [],
      "source": [
        "class BasicNeMoGPTWithSteps(BasicNeMoGPT):\n",
        "\n",
        "    def step_(self, split, batch, batch_idx=None):\n",
        "        idx, targets = batch\n",
        "        logits = self(idx=idx)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        key = 'loss' if split == 'train' else f\"{split}_loss\"\n",
        "        self.log(key, loss)\n",
        "        return {key: loss}\n",
        "\n",
        "    def training_step(self, *args, **kwargs):\n",
        "        return self.step_('train', *args, **kwargs)\n",
        "\n",
        "    def validation_step(self, *args, **kwargs):\n",
        "        return self.step_('val', *args, **kwargs)\n",
        "\n",
        "    def test_step(self, *args, **kwargs):\n",
        "        return self.step_('test', *args, **kwargs)\n",
        "\n",
        "    # This is useful for multiple validation data loader setup\n",
        "    def multi_validation_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
        "        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        return {'val_loss': val_loss_mean}\n",
        "\n",
        "    # This is useful for multiple test data loader setup\n",
        "    def multi_test_epoch_end(self, outputs, dataloader_idx: int = 0):\n",
        "        test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "        return {'test_loss': test_loss_mean}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ki3kRxag511"
      },
      "outputs": [],
      "source": [
        "m = BasicNeMoGPTWithSteps(cfg=cfg.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_7YziAw_Isu"
      },
      "source": [
        "### マルチバリデーションとマルチテストデータローダーの設定",
        "\n",
        "NeMo Primerで説明されているように、NeMoには検証およびテスト段階用の複数のデータローダーが組み込まれています。このようなサポートを追加する方法の一例として、`multi_validation_epoch_end`と`multi_test_epoch_end`のオーバーライドを実装しています。",
        "\n",
        "複数の分散GPUから得られた結果を統合し、エポック終了時に適切に結果を集約することも実務上必須です。NeMoは、たとえ単一デバイスのみで作業する場合でも、結果の正しい統合を厳格に強制します！このケースに対応するため、モデル設計には将来を見据えた設計が組み込まれています！",
        "\n",
        "したがって、NeMoは上記の2つの汎用メソッドを提供し、集約処理をサポートすると同時に複数のデータセットを同時にサポートします！",
        "\n",
        "**注意：既存の`on_validation_epoch_end`と`on_test_epoch_end`実装の先頭に`multi_`を追加するだけで、マルチデータセットとマルチGPUサポートを有効にすることができます！**",
        "\n",
        "------",
        "**注意: マルチデータセット機能を無効化するには、`multi_validation_epoch_end` と `multi_test_epoch_end` の代わりに `on_validation_epoch_end` と `on_test_epoch_end` をオーバーライドしてください！**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpfSn-YUh7GK"
      },
      "source": [
        "## オプティマイザ/スケジューラの設定",
        "\n",
        "私たちはMinGPTモデルと同等の機能をほぼ達成しています！ただし、重要な構成要素であるオプティマイザーが欠けています。",
        "\n",
        "すべての NeMo Model には、デフォルトで `setup_optimization()` が実装されています。この関数は、提供されたモデル設定を解析して `optim` と `sched` のサブ設定を取得し、自動的に最適化アルゴリズムとスケジューラを設定します。",
        "\n",
        "もしGPTの訓練がAdam最適化アルゴリズムをすべてのパラメータに適用し、コサイン減衰スケジュールで重み減衰を行うという単純な設定だけで実現可能だったなら、それだけで訓練設定は完了していたでしょう。",
        "\n",
        "-------",
        "\n",
        "ただし、GPTは単なる単純なモデルではなく、より具体的には、重み行列には重み減衰を適用する必要がありますが、バイアス、埋め込み行列、またはLayerNorm層には適用する必要はありません。",
        "\n",
        "このような特殊なケースに対するNemoのサポート機能は不要となり、代わりにPyTorch Lightningの`configure_optimizers`メソッドを使用して同じ処理を実行できます。",
        "\n",
        "-------",
        "\n",
        "注意：NeMoモデルの場合、`configure_optimizers`は`setup_optimization()`への単純な呼び出しとして実装されており、その後生成されたオプティマイザとスケジューラを返すようになっています。したがって、`configure_optimizer`メソッドをオーバーライドし、オプティマイザの作成を手動で管理することが可能です！",
        "\n",
        "NeMoの目的は、一般的なケースに対して実用的なデフォルト値を提供することであり、追加の柔軟性が必要になった場合には、PyTorch LightningまたはPyTorchのnn.Module自体に自動的に切り替わる仕組みになっています！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgXkZQiVjnOv"
      },
      "outputs": [],
      "source": [
        "class BasicNeMoGPTWithOptim(BasicNeMoGPTWithSteps):\n",
        "\n",
        "     def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('embedding.pos_emb')\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": self.cfg.optim.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=self.cfg.optim.lr, betas=self.cfg.optim.betas)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kARDwthakEQk"
      },
      "outputs": [],
      "source": [
        "m = BasicNeMoGPTWithOptim(cfg=cfg.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB1kwctv2cYv"
      },
      "source": [
        "-----",
        "それでは、最適化アルゴリズムの設定を行いましょう！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K7zh9Cn2s2u"
      },
      "outputs": [],
      "source": [
        "OmegaConf.set_struct(cfg.model, False)\n",
        "\n",
        "optim_config = OmegaConf.create({\n",
        "    'lr': 3e-4,\n",
        "    'weight_decay': 0.1,\n",
        "    'betas': [0.9, 0.95]\n",
        "})\n",
        "\n",
        "cfg.model.optim = optim_config\n",
        "\n",
        "OmegaConf.set_struct(cfg.model, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P31p8ABthsh0"
      },
      "source": [
        "## データセット/データローダーの設定",
        "\n",
        "その結果、MinGPTの実装をほぼ完全に再現することができました。",
        "\n",
        "注意：NeMoモデルには、少なくとも訓練ステップと検証ステップのために、データセットとデータローダーを読み込むためのすべてのロジックを含める必要があります。",
        "\n",
        "これまでは回避するために空の実装を一時的に提供してきましたが、今こそそれを実装しましょう！",
        "\n",
        "-------",
        "\n",
        "**データセットに関する注意**: 以下では、オリジナルの[char-rnnリポジトリ](https://github.com/karpathy/char-rnn)で公開されている非常に小規模なデータセット`tiny_shakespeare`を使用した例を示します。実際には任意のテキストコーパスを使用できます。minGPTで推奨されているデータセットは、http://mattmahoney.net/dc/textdata.htmlで入手可能です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8dlOcZPkxM1"
      },
      "source": [
        "### データセットの作成",
        "\n",
        "NeMoはニューラル型チェックをサポートしており、データセットにも適用可能です！ほとんどの場合、インポート方法が若干変更されるだけで、`collate_fn`の処理方法に1つの違いがあるだけです。",
        "\n",
        "minGPTからデータセット情報を貼り付けるだけで、変更が必要なのはたった2箇所だけです！",
        "\n",
        "-----",
        "この例では、HuggingFaceの`nlp`が提供するデータセットの上に薄いサブクラスを記述していきます！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-fswFkig9t4"
      },
      "outputs": [],
      "source": [
        "from nemo.core import Dataset\n",
        "from torch.utils import data\n",
        "from torch.utils.data.dataloader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z8XuPeClGNm"
      },
      "outputs": [],
      "source": [
        "class TinyShakespeareDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_path, block_size, crop=None, override_vocab=None):\n",
        "\n",
        "      # load the data and crop it appropriately\n",
        "      with open(data_path, 'r') as f:\n",
        "          if crop is None:\n",
        "              data = f.read()\n",
        "          else:\n",
        "              f.seek(crop[0])\n",
        "              data = f.read(crop[1])\n",
        "\n",
        "      # build a vocabulary from data or inherit it\n",
        "      vocab = sorted(list(set(data))) if override_vocab is None else override_vocab\n",
        "\n",
        "      # Add UNK\n",
        "      special_tokens = ['<PAD>', '<UNK>']  # We use just <UNK> and <PAD> in the call, but can add others.\n",
        "      if not override_vocab:\n",
        "        vocab = [*special_tokens, *vocab]  # Update train vocab with special tokens\n",
        "\n",
        "      data_size, vocab_size = len(data), len(vocab)\n",
        "      print('data of crop %s has %d characters, vocab of size %d.' % (str(crop), data_size, vocab_size))\n",
        "      print('Num samples in dataset : %d' % (data_size // block_size))\n",
        "\n",
        "      self.stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "      self.itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "      self.block_size = block_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.data = data\n",
        "      self.vocab = vocab\n",
        "      self.special_tokens = special_tokens\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data) // self.block_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      # attempt to fetch a chunk of (block_size + 1) items, but (block_size) will work too\n",
        "      chunk = self.data[idx*self.block_size : min(len(self.data), (idx+1)*self.block_size + 1)]\n",
        "      # map the string into a sequence of integers\n",
        "      ixes = [self.stoi[s] if s in self.stoi else self.stoi['<UNK>'] for s in chunk ]\n",
        "      # if stars align (last idx and len(self.data) % self.block_size == 0), pad with <PAD>\n",
        "      if len(ixes) < self.block_size + 1:\n",
        "          assert len(ixes) == self.block_size # i believe this is the only way this could happen, make sure\n",
        "          ixes.append(self.stoi['<PAD>'])\n",
        "      dix = torch.tensor(ixes, dtype=torch.long)\n",
        "      return dix[:-1], dix[1:]\n",
        "\n",
        "  @property\n",
        "  def output_types(self):\n",
        "    return {\n",
        "        'input': NeuralType(('B', 'T'), Index()),\n",
        "        'target': NeuralType(('B', 'T'), LabelsType())\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MEMR4TcmP5K"
      },
      "source": [
        "------",
        "ここまでは何も変更する必要がありませんでした。では、型チェックはどのように行われるのでしょうか？",
        "\n",
        "NeMoは`collate`関数の実装自体の中で型チェックを行います！この場合、Dataset内で`collate_fn`をオーバーライドする必要はありませんが、もしオーバーライドする必要がある場合は、**NeMoでは代わりにプライベートメソッド`_collate_fn`をオーバーライドする必要があります**。",
        "\n",
        "その後、データローダーを若干の修正で使用することができます！",
        "\n",
        "**また、Dataset用の`input_types`を実装する必要はありません。これらはモデルへの入力を生成する役割を担っているためです！**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeKXAknenVch"
      },
      "source": [
        "-----",
        "以下のコードベース[char-rnn](https://github.com/karpathy/char-rnn)からTiny Shakespeareデータセットを準備しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwsdXtVzo--t"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvKcDCvIl9-A"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('tiny-shakespeare.txt'):\n",
        "  !wget https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynCwqDu6vK8P"
      },
      "outputs": [],
      "source": [
        "!head -n 5 tiny-shakespeare.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfRL4t9_oS4C"
      },
      "outputs": [],
      "source": [
        "train_dataset = TinyShakespeareDataset('tiny-shakespeare.txt', cfg.model.block_size, crop=(0,         int(1e6)))\n",
        "val_dataset   = TinyShakespeareDataset('tiny-shakespeare.txt', cfg.model.block_size, crop=(int(1e6), int(50e3)), override_vocab=train_dataset.vocab)\n",
        "test_dataset  = TinyShakespeareDataset('tiny-shakespeare.txt', cfg.model.block_size, crop=(int(1.05e6), int(100e3)), override_vocab=train_dataset.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIlCoZDksEDO"
      },
      "source": [
        "### モデルにおけるデータセット/データローダーサポートの設定",
        "\n",
        "これでデータローダーが正常に動作することが確認できました。次はこれをモデル自体の一部として統合しましょう！",
        "\n",
        "これを行うには、NeMo Modelの3つの特殊属性を使用します：`self._train_dl`、`self._validation_dl`、および`self._test_dl`です。DataLoaderを構築したら、これら3つの変数にDataLoaderを割り当ててください。",
        "\n",
        "マルチデータローダーのサポートも同様です！NeMoが自動的に複数のデータローダーの管理を処理してくれます！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVSfIk_-rMSg"
      },
      "outputs": [],
      "source": [
        "class NeMoGPT(BasicNeMoGPTWithOptim):\n",
        "\n",
        "  def _setup_data_loader(self, cfg):\n",
        "    if self.vocab is None:\n",
        "      override_vocab = None\n",
        "    else:\n",
        "      override_vocab = self.vocab\n",
        "\n",
        "    dataset = TinyShakespeareDataset(\n",
        "        data_path=cfg.data_path,\n",
        "        block_size=cfg.block_size,\n",
        "        crop=tuple(cfg.crop) if 'crop' in cfg else None,\n",
        "        override_vocab=override_vocab\n",
        "    )\n",
        "\n",
        "    if self.vocab is None:\n",
        "      self.vocab = dataset.vocab\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=cfg.shuffle,\n",
        "        collate_fn=dataset.collate_fn,  # <-- this is necessary for type checking\n",
        "        pin_memory=cfg.pin_memory if 'pin_memory' in cfg else False,\n",
        "        num_workers=cfg.num_workers if 'num_workers' in cfg else 0\n",
        "    )\n",
        "\n",
        "  def setup_training_data(self, train_data_config: OmegaConf):\n",
        "    self.vocab = None\n",
        "    self._train_dl = self._setup_data_loader(train_data_config)\n",
        "\n",
        "  def setup_validation_data(self, val_data_config: OmegaConf):\n",
        "    self._validation_dl = self._setup_data_loader(val_data_config)\n",
        "\n",
        "  def setup_test_data(self, test_data_config: OmegaConf):\n",
        "    self._test_dl = self._setup_data_loader(test_data_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ait4nLtIxS96"
      },
      "source": [
        "### データセット/データローダー設定の作成",
        "\n",
        "このモデルを設定する最後の手順は、モデル設定ファイル内に `train_ds`、`validation_ds`、`test_ds` の各設定を追加することです！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6zcTqJixOOL"
      },
      "outputs": [],
      "source": [
        "OmegaConf.set_struct(cfg.model, False)\n",
        "\n",
        "# Set the data path and update vocabular size\n",
        "cfg.model.data_path = 'tiny-shakespeare.txt'\n",
        "cfg.model.vocab_size = train_dataset.vocab_size\n",
        "\n",
        "OmegaConf.set_struct(cfg.model, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlvThf7BysyT"
      },
      "outputs": [],
      "source": [
        "train_ds = OmegaConf.create({\n",
        "    'data_path': '${model.data_path}',\n",
        "    'block_size': '${model.block_size}',\n",
        "    'crop': [0, int(1e6)],\n",
        "    'batch_size': 64,\n",
        "    'shuffle': True,\n",
        "})\n",
        "\n",
        "validation_ds = OmegaConf.create({\n",
        "    'data_path': '${model.data_path}',\n",
        "    'block_size': '${model.block_size}',\n",
        "    'crop': [int(1e6), int(50e3)],\n",
        "    'batch_size': 4,\n",
        "    'shuffle': False,\n",
        "})\n",
        "\n",
        "test_ds = OmegaConf.create({\n",
        "    'data_path': '${model.data_path}',\n",
        "    'block_size': '${model.block_size}',\n",
        "    'crop': [int(1.05e6), int(100e3)],\n",
        "    'batch_size': 4,\n",
        "    'shuffle': False,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVVzR6WKyMT5"
      },
      "outputs": [],
      "source": [
        "# Attach to the model config\n",
        "OmegaConf.set_struct(cfg.model, False)\n",
        "\n",
        "cfg.model.train_ds = train_ds\n",
        "cfg.model.validation_ds = validation_ds\n",
        "cfg.model.test_ds = test_ds\n",
        "\n",
        "OmegaConf.set_struct(cfg.model, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd_9_mxS0ET-"
      },
      "outputs": [],
      "source": [
        "# Let's see the config now !\n",
        "print(OmegaConf.to_yaml(cfg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlwSQENU0JxA"
      },
      "outputs": [],
      "source": [
        "# Let's try creating a model now !\n",
        "model = NeMoGPT(cfg=cfg.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Mp4bhH0tR1"
      },
      "source": [
        "-----",
        "すべてのデータローダーが正常に動作しています！やったー！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZHDqCyo6uWd"
      },
      "source": [
        "# モデルの評価 - エンドツーエンドで実施！",
        "\n",
        "データローダーの設定が完了したら、残るはモデルの訓練とテストのみです！このモデルに必要なコンポーネントの大半は既に揃っています - 訓練用、検証用、テスト用データローダー、最適化アルゴリズム、そして訓練・検証・テストの各ステップを実行する型チェック済みの順方向ステップです！",
        "\n",
        "ただし、GPTモデルをゼロから訓練することがこの入門の目的ではないため、代わりにランダムな初期重みを用いてモデルを数ステップ分だけテストすることで、簡易的な動作チェックを行いましょう。",
        "\n",
        "上記により、以下が保証されます：",
        "\n",
        "1) データローダーは意図した通りに動作している",
        "\n",
        "2) 型チェックシステムにより、ニューラルモジュールが順方向処理を正しく実行していることが保証されます。",
        "\n",
        "3) 損失計算が行われ、モデルのエンドツーエンド実行が可能となり、最終的にPyTorch Lightningをサポートします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "johk6Z0e0WEm"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  accelerator = 'gpu'\n",
        "else:\n",
        "  accelerator = 'cpu'\n",
        "\n",
        "trainer = ptl.Trainer(devices=1, accelerator=accelerator, limit_test_batches=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqeeofEr1S8e"
      },
      "outputs": [],
      "source": [
        "trainer.test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqJy7esrA-Ha"
      },
      "source": [
        "# モデルの保存と復元",
        "\n",
        "NeMoは内部的にモデル構成情報、モデルチェックポイント、およびパラメータを管理しています。",
        "\n",
        "NeMoが上記の一般的なガイドラインに従っている限り、`save_to`と`restore_from`メソッドを使用してモデルの保存と復元が可能です！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DksG_-7G1Vbe"
      },
      "outputs": [],
      "source": [
        "model.save_to('gpt_model.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhjoFdCnBWVh"
      },
      "outputs": [],
      "source": [
        "!ls -d -- *.nemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "567txSF0BYXN"
      },
      "outputs": [],
      "source": [
        "temp_model = NeMoGPT.restore_from('gpt_model.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvnfG0kxBfTt"
      },
      "outputs": [],
      "source": [
        "# [ERROR CELL]\n",
        "temp_model.setup_test_data(temp_model.cfg.test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0ckN44YB-1K"
      },
      "source": [
        "-----",
        "\n",
        "うーん、今回はそう簡単ではなかったようですね。非自明なモデルには非自明な問題がつきもんですよ！",
        "\n",
        "注意：NeMoGPTモデルでは、`setup_train_data`ステップ内でself.vocabを設定します。ただし、この設定は訓練データセットによって生成された語彙に依存しています... そして、これはモデル復元時には**復元されません**（`setup_train_data`を明示的に呼び出さない限り！）。",
        "\n",
        "外部データファイルを作成することで保存/復元機能をサポートし、この機能はNeMoでもサポートされています！NeMoの`register_artifact` APIを使用して、外部ファイルを.nemoチェックポイントに添付できるようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Atyoc4NBjEV"
      },
      "outputs": [],
      "source": [
        "class NeMoGPTv2(NeMoGPT):\n",
        "\n",
        "  def setup_training_data(self, train_data_config: OmegaConf):\n",
        "    self.vocab = None\n",
        "    self._train_dl = self._setup_data_loader(train_data_config)\n",
        "\n",
        "    # Save the vocab into a text file for now\n",
        "    with open('vocab.txt', 'w') as f:\n",
        "      for token in self.vocab:\n",
        "        f.write(f\"{token}<SEP>\")\n",
        "\n",
        "    # This is going to register the file into .nemo!\n",
        "    # When you later use .save_to(), it will copy this file into the tar file.\n",
        "    self.register_artifact('vocab_file', 'vocab.txt')\n",
        "\n",
        "  def setup_validation_data(self, val_data_config: OmegaConf):\n",
        "    # This is going to try to find the same file, and if it fails,\n",
        "    # it will use the copy in .nemo\n",
        "    vocab_file = self.register_artifact('vocab_file', 'vocab.txt')\n",
        "\n",
        "    with open(vocab_file, 'r') as f:\n",
        "      vocab = []\n",
        "      vocab = f.read().split('<SEP>')[:-1]  # the -1 here is for the dangling <SEP> token in the file\n",
        "      self.vocab = vocab\n",
        "\n",
        "    self._validation_dl = self._setup_data_loader(val_data_config)\n",
        "\n",
        "  def setup_test_data(self, test_data_config: OmegaConf):\n",
        "    # This is going to try to find the same file, and if it fails,\n",
        "    # it will use the copy in .nemo\n",
        "    vocab_file = self.register_artifact('vocab_file', 'vocab.txt')\n",
        "\n",
        "    with open(vocab_file, 'r') as f:\n",
        "      vocab = []\n",
        "      vocab = f.read().split('<SEP>')[:-1]  # the -1 here is for the dangling <SEP> token in the file\n",
        "      self.vocab = vocab\n",
        "\n",
        "    self._test_dl = self._setup_data_loader(test_data_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn09jsRZDusN"
      },
      "outputs": [],
      "source": [
        "# Let's try creating a model now !\n",
        "model = NeMoGPTv2(cfg=cfg.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQPIPySDD1K0"
      },
      "outputs": [],
      "source": [
        "# Now let's try to save and restore !\n",
        "model.save_to('gpt_model.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YwCJ4xaJ3bU"
      },
      "outputs": [],
      "source": [
        "temp_model = NeMoGPTv2.restore_from('gpt_model.nemo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcxwDIIWKKCQ"
      },
      "outputs": [],
      "source": [
        "temp_model.setup_multiple_test_data(temp_model.cfg.test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3Olm6ZTKRbO"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  accelerator = 'gpu'\n",
        "else:\n",
        "  accelerator = 'cpu'\n",
        "\n",
        "trainer = ptl.Trainer(devices=1, accelerator=accelerator, limit_test_batches =1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QE2SngCKV2p"
      },
      "outputs": [],
      "source": [
        "trainer.test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2HpKzwKJ_MW"
      },
      "source": [
        "------",
        "これで完了です！これで、外部語彙ファイルがある場合でも、モデルのシリアライズとデシリアライズが問題なく行えるようになりました！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjCV5u3_OO7a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "01_NeMo_Models.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}